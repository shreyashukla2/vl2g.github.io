<!DOCTYPE html>
<!-- saved from url=(0044)http://malllabiisc.github.io/resources/kvqa/ -->
<html lang="en"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="VisToT">
    <meta name="author" content="VL2G IIT J">

    <title>RetVQA</title>

    <link href="./index_files/bootstrap.css" rel="stylesheet">
    <link href="./index_files/style.css" rel="stylesheet">

    <!-- HTML5 shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!--[if lt IE 9]>
      <script src="js/html5shiv.js"></script>
      <script src="js/respond.min.js"></script>
    <![endif]-->
  </head>

  <body>

    <div class="container">
      <div class="header">
        <h2 class="text"><center>Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering</center></h2>
<h4 class="text"><center><a href="https://abhiram4572.github.io/">Abhirama Subramanyam Penamakuri</a><sup>1</sup>, <a href="https://sites.google.com/view/manishg/">Manish Gupta</a><sup>2</sup>, <a href="https://scholar.google.co.in/citations?user=79PCaM0AAAAJ&hl=en">Mithun Das Gupta</a><sup>2</sup>, <a href="https://anandmishra22.github.io/">Anand Mishra</a><sup>1</sup> </center></h4>, 
<h4 class="text"><center><sup>1</sup>Indian Institute of Technology Jodhpur&ensp;<sup>2</sup>Microsoft, India</center></h4>
<h4 class="text"><center><a href="http://ijcai-23.org">IJCAI 2023</a></center></h4>
<h4 class="text"><center> [<a href="https://www.ijcai.org/proceedings/2023/0146.pdf">Paper</a>] [<a href="https://arxiv.org/abs/2306.16713">arxiv</a>] [<a href="https://docs.google.com/presentation/d/1sOUevj65X7FRbzIr2GY1xf8v7t7Xn8RPolfEE2MbIII/edit?usp=sharing">Slides</a>] [<a href="https://docs.google.com/presentation/d/1gfVPvY9-DNQI6GXPmkz95Bll2WJpOegMCQjl_AxKa-I/edit?usp=sharing">Poster</a>] [<a href="https://youtu.be/ouE7OHFdU90">Short talk</a>] [<a href="https://drive.google.com/file/d/1j08lIXSN5Uxn5imHKXn4JIrzq5RitE04/view?usp=share_link">Data</a>] </center></h4>

<!-- <h4 class="text"><center> [<a href="https://drive.google.com/file/d/1yANgw3vPpnwRiGKMPjIWWt_kzEMEPjb_/view?pli=1">Pre-print</a>] [<a href="#">Paper</a>]  [<a href="#">Code</a>]  [<a href="#">Poster</a>]  [<a href="#">Short Talk</a>]  [<a href="#">Slides</a>]</center></h4> -->
       </div>

 <div class="container">
      <div class="header">
      <br>
<center>
<figure class="figure"> 
    <img class="figure-img" width="60%" src= "figures/goal_ijcai.pdf" > 
 </figure>
</center>
&nbsp;
&nbsp;
&nbsp;

</div>
      <div class="row">
        <h3>Abstract</h3>
        <p style="text-align: justify;">                   
          We study visual question answering in a setting where the answer has to be mined from a pool of relevant and irrelevant images given as a context. For such a setting, a model must first retrieve relevant images from the pool and answer the question from these retrieved images. We refer to this problem as retrieval-based visual question answering (or RETVQA in short). The RETVQA is distinctively different and more challenging than the traditionally studied Visual Question Answering (VQA), where a given question has to be answered with a single relevant image in context. Towards solving the RETVQA task, we propose a unified Multi Image BART (MI-BART) that takes a question and retrieved images using our relevance encoder for free-form fluent answer generation. Further, we introduce the largest dataset in this space, namely RETVQA, which has the following salient features: multi-image and retrieval requirement for VQA, metadata-independent questions over a pool of heterogeneous images, expecting a mix of classification-oriented and open-ended generative answers. Our proposed framework achieves an accuracy of 76.5% and a fluency of 79.3% on the proposed dataset RETVQA and also outperforms state-of-the-art methods by 4.9% and 11.8% on the image segment of the publicly available WebQA dataset on the accuracy and fluency metrics, respectively. 
	</p>
      </div>
      <div class="row">
        <h3>Highlights</h3>
     <ul> 
     <li> We introduce RetVQA, the largest dataset in this space with multi-image and retrieval requirement for VQA.</li>
     <li> We propose a unified Multi Image BART (MI-BART) that takes a question and retrieved images using our relevance encoder for free-form fluent answer generation. </li>
     <!-- <li> MI-BART achieves an accuracy of 76.5% and a fluency of 79.3% on the proposed dataset RETVQA and also outperforms state-of-the-art methods by 4.9% and 11.8% on the image segment of the publicly available WebQA dataset on the accuracy and fluency metrics, respectively.</li> -->
     </ul>
               
     </div>


     <div class="row">
      <h3 id="datasetD">Code and Data</h3>
      Coming Soon.
      <!-- <b>Explore COFAR dataset:</b> [<a href="./gallery/index.html">Gallery</a>]<br> -->
      <!-- <div class="row">
        <ol type="A">
          <li>Dataset Images&nbsp;[<a href="https://drive.google.com/file/d/1pzQdDhCCLWn-L5VMxBb2s4rY7M7mQkdf/view?usp=sharing"><i>Image URLs</i></a>] (585 KB)</li>
          <li>Train Data&nbsp;[<a href="https://drive.google.com/file/d/1VxPYVQjvDnIXS6iRV2q8F72Cp3O2UcYx/view?usp=sharing"><i>Image-Caption pairs</i></a>] (313 KB)</li>

        </ol>
      </div> -->
    </div>

<hr>

<h3><strong><span style="font-size: 14pt;">Bibtex</span></strong></h3>
<p>Please cite our work as follows:</p>
<pre><tt>@inproceedings{retvqa2023ijcai,
  author    = "Penamakuri, Abhirama S. and 
              Gupta, Manish and
              Das Gupta, Mithun and 
              Mishra, Anand"
  title     = "Answer Mining from a Pool of Images: Towards Retrieval-Based Visual Question Answering",
  booktitle = "IJCAI",
  year      = "2023",
}</tt></pre>
     </div>

<hr>
<h3><strong><span style="font-size: 14pt;">Acknowledgements</span></strong></h3>
Abhirama S. Penamakuri is supported by Prime Minister Research Fellowship (PMRF), Minsitry of Education, Government of India.<br> 
We thank Microsoft for supporting this work through the Microsoft Academic Partnership Grant (MAPG) 2021.
<br><br><br>      
